## Tres partes
1- Obtendo os dados:
    * Api
    * Curl + html (bs4)
    * Selenium + html (bs4)
2- Manipular os dados:
    * Pandas + Python
    * Scikit
3- Apresentação
    * Jupyter
    * Slides
    * HTML + CSS
    * matplotlib, wordcloud

# NOTES
1-
    Para obter dados da internet temos 3 opcoes em ordem de facilidade:
        a. API (se existir)
        b. CURL da pagina (se a pagina for estatica)
        c. Browser automatizado (se a pagina for javascripteira)
    Depois de obtido o HTML ainda precisamos parsear ele pra extrair os dados que queremos.
    Vale a pena fazer isso em dois passos nesse caso. Salvando o html bruto e depois extraindo um csv dele

2- 



# TODO
* Fazer uma introducao
* Fazer mais NOTES
* crawler esta clicando em imagens e bugando
* checar quebra de linha no crawler
